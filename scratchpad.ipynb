{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eee20d07",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "BASELINELD_PATH = os.path.join(\"/well\", \"palamara\", \"projects\", \"S-LDSC_reference_files\", \"GRCh38\", \"baselineLD_v2.2\")\n",
    "PLINK_PATH = os.path.join(\"/well\", \"palamara\", \"projects\", \"S-LDSC_reference_files\", \"GRCh38\", \"plink_files\")\n",
    "TRAITGYM_PATH = os.path.join(\"/well\", \"palamara\", \"users\", \"nrw600\", \"contribution_prediction\", \"TraitGym\", \"results\", \"dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34068aa8",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "def setup_logger(seed):\n",
    "    \"\"\"set useful logger set-up\"\"\"\n",
    "    logging.basicConfig(\n",
    "        format=\"%(asctime)s %(message)s\", encoding=\"utf-8\", level=logging.INFO\n",
    "    )\n",
    "    # logging.debug(f\"Pytorch version: {torch.__version__}\")\n",
    "    if seed is not None:\n",
    "        logging.info(f\"Seed: {seed}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85e51f23",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def load_baselineLD_annotations(file_path):\n",
    "    \"\"\"\n",
    "    Load baseline LD annotations from a predefined file path.\n",
    "\n",
    "    Returns:\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(file_path, sep=\"\\t\", compression=\"gzip\")\n",
    "    logging.info(f\"Loaded {len(df)} rows from {file_path}\")\n",
    "    logging.info(f\"Columns in the dataframe: {df.columns.tolist()}\")\n",
    "    logging.info(f\"Dataframe head:\\n{df.head()}\")\n",
    "    logging.info(f\"Data types:\\n{df.dtypes}\")\n",
    "    return df\n",
    "\n",
    "\n",
    "def load_bim_file(file_path):\n",
    "    \"\"\" \"\"\"\n",
    "\n",
    "    bim_cols = [\"chrom\", \"SNP\", \"genetic_dist\", \"pos\", \"ref\", \"alt\"]\n",
    "    bim_df = pd.read_csv(file_path, sep=\"\\t\", header=None, names=bim_cols)\n",
    "\n",
    "    return bim_df\n",
    "\n",
    "\n",
    "def merge_ld_bim(ld_df, bim_df):\n",
    "    \"\"\"\n",
    "    Merge LD and BIM dataframes on SNP column.\n",
    "\n",
    "    Returns:\n",
    "    \"\"\"\n",
    "    merged_df = pd.merge(ld_df, bim_df, on=\"SNP\", how=\"inner\")\n",
    "    return merged_df\n",
    "\n",
    "\n",
    "def load_traitgym_data(file_path, split):\n",
    "    \"\"\"\n",
    "    Load TraitGym dataset splits\n",
    "\n",
    "    Returns:\n",
    "    \"\"\"\n",
    "    df = pd.read_parquet(file_path)\n",
    "    logging.info(f\"Loaded {len(df)} rows from {file_path}\")\n",
    "    logging.info(f\"Columns in the dataframe: {df.columns.tolist()}\")\n",
    "    logging.info(f\"Dataframe head:\\n{df.head()}\")\n",
    "    logging.info(f\"Data types:\\n{df.dtypes}\")\n",
    "    return df\n",
    "\n",
    "\n",
    "def merge_varient_features(traitgym_df, annotation_df):\n",
    "    \"\"\" \"\"\"\n",
    "    for df in (traitgym_df, annotation_df):\n",
    "        df[\"chrom\"] = df[\"chrom\"].astype(str)\n",
    "        df[\"pos\"] = df[\"pos\"].astype(int)\n",
    "        df[\"ref\"] = df[\"ref\"].astype(str)\n",
    "        df[\"alt\"] = df[\"alt\"].astype(str)\n",
    "    merged_df = pd.merge(\n",
    "        traitgym_df, annotation_df, on=[\"chrom\", \"pos\", \"ref\", \"alt\"], how=\"inner\"\n",
    "    )\n",
    "    return merged_df\n",
    "\n",
    "\n",
    "def load_data(chromosome):\n",
    "    \"\"\"\n",
    "    Load data for a specific chromosome.\n",
    "\n",
    "    Returns:\n",
    "    \"\"\"\n",
    "    ld_annotations = load_baselineLD_annotations(\n",
    "        os.path.join(BASELINELD_PATH, f\"baselineLD.{chromosome}.annot.gz\")\n",
    "    )\n",
    "    bim_file = load_bim_file(\n",
    "        os.path.join(PLINK_PATH, f\"1000G.EUR.hg38.{chromosome}.bim\")\n",
    "    )\n",
    "    merged_data = merge_ld_bim(ld_annotations, bim_file)\n",
    "\n",
    "    traitgym_data = load_traitgym_data(\n",
    "        os.path.join(TRAITGYM_PATH, \"mendelian_traits_all\", f\"test.parquet\"),\n",
    "        split=\"test\",\n",
    "    )\n",
    "    merged_traitgym_data = merge_varient_features(traitgym_data, merged_data)\n",
    "    convert_columns = {\n",
    "        \"chrom\": int,\n",
    "        \"ref\": str,\n",
    "        \"alt\": str,\n",
    "        \"OMIM\": str,\n",
    "        \"consequence\": str,\n",
    "        \"SNP\": str,\n",
    "    }\n",
    "    for col, dtype in convert_columns.items():\n",
    "        if col in merged_traitgym_data.columns:\n",
    "            merged_traitgym_data[col] = merged_traitgym_data[col].astype(dtype)\n",
    "        else:\n",
    "            logging.warning(f\"Column {col} not found in merged_traitgym_data\")\n",
    "    logging.info(\n",
    "        f\"Object columns:\\n{merged_traitgym_data.select_dtypes(include='object').columns.tolist()}\"\n",
    "    )\n",
    "\n",
    "    return merged_traitgym_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd08951c",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from utils import setup_logger\n",
    "\n",
    "setup_logger(seed=42)  # Initialize logger with a seed for reproducibility\n",
    "\n",
    "data = data_loading.load_data(chromosome=11)  # Load data for chromosome 1\n",
    "\n",
    "logging.info(\"Data loaded successfully.\")\n",
    "logging.info(f\"Data shape: {data.shape}\")\n",
    "logging.info(f\"Columns: {data.columns.tolist()}\")\n",
    "logging.info(f\"Column types:\\n{data.dtypes}\")\n",
    "logging.info(\n",
    "    f\"Object columns:\\n{data.select_dtypes(include='object').columns.tolist()}\"\n",
    ")\n",
    "\n",
    "X = data.drop(columns=[\"label\"])  # Features\n",
    "y = data[\"label\"]  # Target variable\n",
    "\n",
    "# Split data\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X, y, stratify=y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Train XGBoost model\n",
    "model = xgb.XGBClassifier(use_label_encoder=False, eval_metric=\"logloss\", enable_categorical=True)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict\n",
    "y_pred = model.predict_proba(X_val)[:, 1]\n",
    "\n",
    "# Evaluate\n",
    "ap = average_precision_score(y_val, y_pred)\n",
    "print(f\"AUPRC: {ap:.3f}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
